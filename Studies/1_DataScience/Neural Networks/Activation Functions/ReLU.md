A Rectified Linear Activation Function (ReLU) in the context of neural networks is a function to introduce the property of non-linearity to the output of deep learning models, it can solve the vanishing gradient issue.

Graphical representation of $$f(x) = Max(0,x)$$
![[Pasted image 20230927111935.png]]

The idea is to preserve values that are greater then 0. And lower values are represented by 0, meaning that in the learning process they have no impact.